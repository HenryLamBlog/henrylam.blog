<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Colorization of Grayscale Images</title>
        <!-- Include any necessary CSS files -->
        <link href="https://fonts.googleapis.com/css?family=Space+Mono" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet">
        <link rel="stylesheet" href="css/animate.css">
        <link rel="stylesheet" href="css/icomoon.css">
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/style.css">
        <style>
            /* Adjust the background color to the orange theme */
            body {
                font-family: 'Space Mono', monospace;
                background-color: #f8f9fa;
                margin: 0;
                overflow-x: hidden;
            }
            /* Adjust the spacing and margins used in the rest of your page */
            .project-details {
                padding: 100px;
                margin: 0 auto;
                width: 60%;
                position: relative;
                
            }
    
            /* Sticky Navigation Bar */
              .navbar {
                display: flex;
                align-items: center; /* Align items vertically */
                justify-content: space-between;
                background-color: #FF9000 !important; /* Background color of the navbar */
                padding: 10px 20px; /* Adjust padding as needed */
                z-index: 9999;
                /*height: 80px; /* Set a fixed height for the navbar */
              }
            .navbar-brand h1 {
                font-size: 24px; /* Adjust the font size of the brand */
                color: #ffffff; /* Color of the brand text */
                margin: 0; /* Remove any default margin */
            }


            .navbar-nav .nav-link {
                color: #ffffff; /* Color of the nav links */
                font-size: 18px; /* Font size of the nav links */
                display: flex;
                align-items: center; /* Align items vertically */
            }


            
    
            /* Style the headings and paragraphs */
            h1, h2, h3 {
                color: #3a170f; /* Adjust the color to match your theme */
                font-family: 'Space Mono', monospace;
            }
    
            p {
                color: #666; /* Adjust the color to match your theme */
            }
    
            /* Style the PDF container */
            .pdf-container {
                margin-top: 20px;
                padding: 20px;
                box-sizing: border-box;
                border: 1px solid #FF9000; /* Adjust the border color to match your theme */
                border-radius: 5px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
                background-color: #fff; /* Match the background color with your theme */
            }
    
            /* Style the embedded PDF */
            .pdf-container embed {
                width: 100%;
                height: 600px;
            }
    
            img.lab-gif {
                max-width: 100%;
                height: auto;
                display: block;
                margin: 20px auto;
            }
            .lab-text {
                text-align: center;
                font-style: italic;
                color: #888;
            }
        
  
        </style>
    </head>
    <body>
    <!-- Navbar -->
    <nav class="navbar navbar-expand-sm sticky-top navbar-light" id="main-navbar">
        <div class="container">
            <!-- Navbar brand -->
            <a class="navbar-brand" href="/">
                <h1>Henry Lam</h1>
            </a>

            <!-- Navbar toggler button for mobile -->
            <!-- <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button> -->

            <!-- Links -->
            <div class="collapse navbar-collapse" id="navbar">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a id="about-nav" class="nav-link theme2" href="index.html#about">About Me</a>
                    </li>
                    <li class="nav-item">
                        <a id="resume-nav" class="nav-link theme2" href="index.html#resume">My Resume</a>
                    </li>
                    <li class="nav-item">
                        <a id="projects-nav" class="nav-link theme2" href="index.html#projects">Projects</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
        
    <!-- Include your header/navigation if needed -->

    <!-- Content specific to the Colorization of Grayscale Images project -->
    <div class="project-details">
        <h1>Comparative Analysis of Colorization Techniques for Grayscale Images</h1>
        <p>In this project, we conducted a comparative analysis of colorization techniques for grayscale images, exploring methods such as Deep Learning, Support Vector Regression, and Probability-Based Methods.</p>

        <h2>Project Description</h2>
        <p>Four different methods for image colorization, namely support vector machines (SVM), Parzen windows (PWM), transfer learning, and conditional generative adversarial networks (cGAN) were implemented. The effectiveness of these methods was evaluated based on their ability to produce accurate and realistic colorized images, as well as their performance on various metrics such as mean absolute error (MAE), root mean square error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM).</p>
        
        <!-- Include more project details as needed -->
        <h2>Methods</h2>
        <p>The methods that we propose to use for the automatic image colorization are itemized below:</p>
        <ul>
            <li>Deep Learning</li>
            <li>Support Vector Regression</li>
            <li>Convolutional Neural Network</li>
            <li>Conditional Generative Adversarial Network</li>
        </ul>
        <p></p>
        <h2>Dataset Information</h2>
        <p>Our dataset consists of 7,129 images depicting various scenes such as streets, buildings, mountains, glaciers, and trees. The dataset includes both color and grayscale versions of these images, facilitating direct comparisons across different methods.</p>
        <p>Most images in the dataset have dimensions of 150 × 150 pixels, although some are smaller. We divided the dataset randomly, with 80% of the images designated as training data and the remaining 20% as testing data.</p>
        <p>To ensure unbiased evaluation, the same distribution of training and testing images is maintained across all three methods. However, for SVR and PWM, only a subset of images is utilized for training due to scalability issues with larger training sets.</p>
        <h3>LAB Conversion</h3>
        <p>Accurately measuring color similarity and associating saturated colors with corresponding grayscale levels are crucial in image colorization.</p>
        <p>In the classic RGB color space, every pixel is represented by three color values: Red, Green, and Blue. Hence, in an 8-bit image, each channel denoted (R, G, B) ranges from 0 to 255. Grayscale images, on the other hand, have only the intensity value for each pixel, resulting in a monochromatic image representation without any color information.</p>
        <img src="images/labcolor.gif" alt="LAB 3D-space" class="lab-gif">
        <p class="lab-text">LAB 3D-space</p>
        <p>By utilizing LAB color space, it has the ability to separate the luminance or lightness component (L channel) from the color components (a and b channels). The L channel solely represents the grayscale axis, capturing only the intensity or brightness information of the image from 0 to 100. On the other hand, the a and b channels encode the orthogonal color axes, representing the green-red and blue-yellow color information from -128 to 128, respectively.</p>
        

        <!-- Include more details about the methods -->

        <h2>Comparison & Validation Methods</h2>
        <p>Quantitative metrics such as MAE, RMSE, SSIM, and PSNR can assess structural differences by converting images to the YCbCr color space and using the Luma channel as the input. We will also employ cross-validation techniques, such as k-fold validation, which can help ensure unbiased performance assessment by training and testing the models on different subsets of the dataset.</p>

        
        <h3>Support Vector Regression</h3>
        <p>The Support Vector Regression (SVR) method utilizes a subset of the 80% training data due to SVR's poor scalability with the number of samples. Different percentages of training images (5%, 10%, and 20%) are employed to compare the method's quality for varying numbers of training images. During training, each colored image undergoes smoothing with a 3×3 Gaussian kernel (σ = 1), followed by segmentation using the SLIC superpixel algorithm to group similar grayscale brightness segments. After segmentation, the images are converted to LAB color space, and the scales of the a and b channels are adjusted to range between -1 and 1. A 2D Fourier transformation is applied to the L channel around the center of each segment, generating arrays for each segment along with the average a and b for every training image.

        The SVR model is trained using the sklearn.svm.SVR module, where the average a and b of each segment of each image are fitted against the array containing 2DFFT for each segment. The trained model is then used to predict the a and b channels of each segment in the test images. Attempts to implement Markov Random Field to smooth across adjacent segments were made but did not improve the results' quality and were consequently excluded from the process.</p>

        <h3>Parzen Window Methods & Spatial Coherence</h3>
        <p></p>

        <h3>Convolutional Neural Network</h3>
        <p></p>

        <h3>General Adversarial Network</h3>
        <p></p>

        <h2>Key differences</h2>
        <p>One of the key differences noted in this paper was the different requirements for training volume between the deep learning methods and the non-deep learning methods. The SVM, for instance, needed to be trained at a reduced value of already limited pictures, whereas the limited available images in the dataset actually reduced the effectiveness of the transfer learning network, potentially contributing towards its premature convergence.
        </p>

        <h2>Results</h2>
        <p></p>

        <h2>Comparison in Metrics Between Methods</h2>
        <div class="table-responsive">
            <table class="table table-bordered">
                <thead>
                    <tr>
                        <th></th>
                        <th>SVR</th>
                        <th>SVR-STD</th>
                        <th>PWM</th>
                        <th>PWM-STD</th>
                        <th>CNN</th>
                        <th>CNN-STD</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Mean Absolute Error A</td>
                        <td>7.23</td>
                        <td>4.42</td>
                        <td>6.64</td>
                        <td>2.58</td>
                        <td>5.75</td>
                        <td>4.49</td>
                    </tr>
                    <tr>
                        <td>Mean Absolute Error B</td>
                        <td>15.6</td>
                        <td>7.02</td>
                        <td>16.6</td>
                        <td>8.40</td>
                        <td>13.1</td>
                        <td>6.61</td>
                    </tr>
                    <tr>
                        <td>Root Mean Square Error A</td>
                        <td>8.61</td>
                        <td>4.97</td>
                        <td>9.37</td>
                        <td>3.26</td>
                        <td>7.35</td>
                        <td>5.14</td>
                    </tr>
                    <tr>
                        <td>Root Mean Square Error B</td>
                        <td>18.6</td>
                        <td>7.92</td>
                        <td>21.6</td>
                        <td>8.83</td>
                        <td>16.1</td>
                        <td>7.25</td>
                    </tr>
                    <tr>
                        <td>SSIM</td>
                        <td>0.91</td>
                        <td>0.06</td>
                        <td>0.74</td>
                        <td>0.10</td>
                        <td>0.94</td>
                        <td>0.05</td>
                    </tr>
                    <tr>
                        <td>PSNR</td>
                        <td>29.9</td>
                        <td>0.81</td>
                        <td>19.1</td>
                        <td>3.26</td>
                        <td>23.0</td>
                        <td>3.83</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <h2>Conclusion</h2>
        <p>Broadly, two different techniques were applied: deep learning and non-deep learning methods. The SVR required reduced training volume compared to other methods, but the limited available images reduced the effectiveness of the transfer learning network.</p>
        <p>Images colorized by the Probabilistic-diffusion and SVR methods exhibited color issues upon generation. Larger training datasets did not always correlate with better results, raising questions about method appropriateness for large datasets.</p>
        <p>The CNN deep learning model outperformed others on most metrics, indicating the need for further research. However, other techniques may be more beneficial with limited data. The cGAN, though not fully implemented, warrants further exploration.</p>
        <h2>Next Steps</h2>
        <p>For SVR, improving scalability and quality through sklearn.svm.LinearSVR and Markov Random Fields implementation is suggested.</p>
        <p>PWM could benefit from pixel-to-section level approaches and coherence optimization.</p>
        <p>CNN requires fine-tuning, hyperparameter tuning, data augmentation, and diverse dataset incorporation to enhance colorization.</p>
        <p>cGAN evaluation against other methods and consideration of training data quantity are essential for further development.</p>

        <h2>Group</h2>
        <ul>
            <li>Gummesen, Thomas Skøtt</li>
            <li>Lam, Hui Yin (Henry)</li>
            <li>Pepping, Bartholomeus Diederik Rasmussen </li>
            <li>Magne Egede</li>
        </ul>


        
    </div>

    <div id="fh5co-footer">
		<div class="container">
			<div class="row">
				<div class="col-md-12">
					<p>&copy; henrylam.blog <br>Made by Henry Lam</p>
				</div>
			</div>
		</div>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-arrow-up22"></i></a>
	</div>
    <!-- Include any necessary JavaScript files -->
    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>
    <!-- jQuery Easing -->
    <script src="js/jquery.easing.1.3.js"></script>
    <!-- Bootstrap -->
    <script src="js/bootstrap.min.js"></script>
    <!-- Waypoints -->
    <script src="js/jquery.waypoints.min.js"></script>
    <!-- Stellar Parallax -->
    <script src="js/jquery.stellar.min.js"></script>
    <!-- Easy PieChart -->
    <script src="js/jquery.easypiechart.min.js"></script>
    <!-- Google Map -->
    <script src="https://maps.googleapis.com/maps/api/js?v=3.exp&amp;sensor=false"></script>
    <script src="js/google_map.js"></script>
    <!-- Main -->
    <script src="js/main.js"></script>
</body>
</html>
